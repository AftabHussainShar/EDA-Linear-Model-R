---
title: "EDA-Linear-Model"
author: "Put your name"
date: ""
output: 
  html_document: default
  pdf_document: default
  word_document: default
---
<style type="text/css">
  body{
  font-size: 8pt;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(error = TRUE)
```

```{r, echo=FALSE,results='hide',warning=FALSE,message=FALSE}
library(knitr)
library(tidyverse)
library(stringr)
library(ggplot2)
library(GGally)
library(reshape2)
library(naniar)
library(car)
library(DMwR2)
library(modelr)
```

```{r, package_options, echo=FALSE, cache=FALSE, results='hide', warning=FALSE, comment=FALSE}
rm(list = ls())
cat("\014")
for(i in 1:2){
  Wrkng_Dir <- setwd(dirname(rstudioapi::getActiveDocumentContext()$path))}
Wrkng_Dir
```

#### 1. Title and abstract:
Among data scientists, the Ames Housing Data Set is well-liked. The goal for each person is to minimize the RMSE value on the test set. Similar to that, in this paper we use a variety of exploratory data analysis techniques to analyze the information and develop an attractive linear model with a train and test data-set that includes 81 features that describe a wide range of attributes of 1,460 Ames residences sold between 2006 and 2010. In addition, five issues are included in this study that are recognized as components of the SalePrice prediction process based on different explanatory characteristics. With the data gathered from multiple websites, we are able to pinpoint here the salient characteristics that are actually influencing SalePrice. At the conclusion of the project, a list of all the websites that were referenced.To further guarantee the robustness and dependability of the model, we carry out a comprehensive assessment of its performance, which includes residual analysis and cross-validation. Our findings support educated decision-making by offering insightful information to real estate agents, legislators, and potential homeowners alike.


#### 2.Problem Identification Part 1:
Loading and understanding the data.
```{r , echo=TRUE,results='hide',echo=TRUE,results='hide',message=FALSE,warning=FALSE}
Ames_train <- read.csv("train.csv",na.strings=c(""," ","NA"))
Ames_test <- read.csv("test.csv",na.strings=c(""," ","NA"))
```

```{r , echo=FALSE,results='hide',message=FALSE,warning=FALSE}
str(Ames_train)
str(Ames_test)
```
The Ames Housing dataset comprises 80 variables, with SalePrice as our target feature of interest. Among these, 79 explanatory variables primarily focus on quantifying and assessing numerous physical attributes of the properties. These attributes typically include factors such as construction date, property size, living area square footage, parking availability, bathroom and bedroom count, flooring and roofing materials, and property location. Several continuous variables provide data on various dimensions of the properties, such as LotArea, PoolArea, and GarageArea. Additionally, categorical variables describe the quality and type of amenities, materials used in construction or renovation, street or neighborhood characteristics, and nearby amenities.Discrete variables detail the number and location of amenities, bedrooms, bathrooms, and kitchens within each property. Furthermore, temporal variables indicate the year of renovation, garage construction, and property construction.

The data types of the columns are mixed: we have integers, numric data & factors
(levels). So, it’s clear that features come in fundamentally different types :
1. Some features are inherently NUMERICAL. They are quantities that we can measure or
count. Some of these are continuous, such as the total living area (GrLivArea), while
others are discrete, such as the number of rooms (TotRmsAbvGrd).
2. Other features are CATEGORICAL. They are qualitative or descriptive in nature. For
example, this includes the neighbourhood in which the house is located
(Neighborhood), and the type of foundation the house was built on (Foundation). There
is no inherent ordering to these features.
3. Yet others are ORDINAL. They comprise categories with an implicit order. Examples of
this include the overall quality rating (OverallQual) or the irregularity of the lot
(LotShape). We can think of them as representing values on an arbitrary scale

Thorough examination, several key features significantly influence SalePrice. These can be summarized as the size of the property, the number of rooms, location, available amenities, construction materials, overall age of the property, and the condition of both the property and its amenities.

#### 3. Problem Identification Part 2: 

##### These are the few DataScience problems, we came across while studying dataset and affect SalePrice based on the key features. We will find solution to each one of them once we finish data Analysis based on several questions which we think of.
######  1. Problem 1: Identify which suburb/location had the biggest growth in SalePrice by plotting and examining the sale prices cross different suburbs.Has there been a trend on the type of house bought and had big hike in Sale price from 2006 to 2010
######  2. Problem 2:	Analyze a possible pattern of SalePrice vs YrSold/MoSold, LotArea and/or some other variables which can reasonably be included considering Totl_Area instead of LotArea,SeasonSold instead of MonthSold here.
######  3. Problem 3: Whether SaleCondition has any impact on the SalePrice,  Explain with Data Analysis and give insights on whether this feature needs to be considered.
######  4. Problem 4 : Any change in SalePrice over the period  from 2006 to 2010 based on GarageQual
######  5. Problem 5:Over the years.How the SalePrice changed based on Neighborhood and BldgType .Explain


#### 4. Data Preprocessing:

Fitting a model that could indicate that there is zero error in the training data appears to be quite simple. That kind of model, however, would be extremely subpar since it fails to define the relationship between the explanatory features and the target variable, sales price. Fitting a linear model with the lowest RMSE and highest R-squared value is the primary goal here.

Here is the cyclic process wherein started with data exploration followed by exploratory data analysis and missing value detection and imputation. Basically, during data exploration process, one need to thoroughly study the data and relationship to the target feature. Once finding linear relationship, next step is to check the correlation values of numeric features. Here this stage answers to the question , which variables are most strongly correlated with the response. These set of numeric features will be the strongest predictor of the SalePrice. Correlation heatmap is used here to capture correlation values.

Deep hues in the heatmap typically indicate a strong correlation. The linear modeling is directly impacted by missing values. The model's quality decreases with the amount of missing values.  A significant percentage of missing values indicates that the feature is essentially absent from the property. The missing value proportion of the characteristics is also displayed in the plots. For instance, there will be differences in the correlation to the SalePrice because the houses with and without pools undoubtedly account for the missing values. These NAs are imputed with the median value for numerical attributes and are marked as "Missing" in categorical features.

In the feature engineering stage, Saleprice is trained using a logarithmic transformation.  Otherwise, SalePrice had a couple outiliers, which was a reasonable tilt. While some features were ordinal, others were strings. These were transformed into numerical levels so that further data could be added to the models. Next, using temporal features, it was possible to impute all year-based columns—aside from YearSold—in order to indicate the property's and garage's ages. This demonstrated the linearity of SalePrice. In order to verify the relationship between the SalePrice and the MonthSold, a SeasonSold column was also developed. In addition, a new column is established that includes the total area of the basement plus the ground living area. With this new feature, it is possible to find linearity with SalePrice once more.

The 5 problems which are defined above are also solved later on. Linear modeling with the best features selected by deeper study into the data set using Exploratory data analysis helped further to choose and find the best model.

I was able to explore four models with varying but very near high R-squared values around 0.8 thanks to the knowledge I gained during the data analysis process. I primarily chose the features for modeling based on the good variability of the explanatory variables and the high correlation values.and lastly fitted the curve and trained the model. The prediction is examined using the test data set, and it is discovered that, out of the three models with the lowest R-squared value, the selected model is the best.


#### 5. Exploratory Data Analysis:

##### 1. Every data analysis starts with checking on the distrubution statics of the target feature. The distribution of SalePrice looks skewed positive, and have outliers. We could consider log    transformation in such a case.
```{r , echo=FALSE, cache=FALSE,fig.width=20,fig.height=8, results=TRUE, warning=FALSE, comment=FALSE, warning=FALSE}
ggplot(Ames_train, aes(x = SalePrice, fill = ..count..)) + 
  geom_histogram(bins=15) +
  xlab("Sale Price $") +
  ylab("Number observations") +
  ggtitle("Histogram of sale prices")
```

```{r , echo=FALSE, cache=FALSE, results=FALSE, warning=FALSE, comment=FALSE, warning=FALSE}
outliers_SP <- boxplot.stats(Ames_train$SalePrice)$out
outliers_SP
Ames_train$log_SalePrice <- log(Ames_train$SalePrice)
Ames_test$log_SalePrice <- log(Ames_test$SalePrice)
```
  The log transformation of SalePrice looks like normally distributed now.
```{r , echo=FALSE, cache=FALSE, fig.width=20,fig.height=8,results=TRUE, warning=FALSE, comment=FALSE, warning=FALSE}
ggplot(Ames_train, aes(x = log_SalePrice, fill = ..count..)) + 
  geom_histogram(bins=15) +
  xlab("Sale Price $") +
  ylab("Number of observations") +
  ggtitle("Histogram of sale prices")
```

```{r , echo=FALSE, cache=FALSE, results=FALSE, warning=FALSE, comment=FALSE, warning=FALSE}
Ames_train_tidied <- data.frame(Ames_train) 
Ames_train_tidied[sapply(
  Ames_train_tidied, is.character)] <- lapply(
    Ames_train_tidied[sapply(Ames_train_tidied, is.character)], 
    as.factor)
Ames_test[sapply(
  Ames_test, is.character)] <- lapply(
    Ames_test[sapply(Ames_test, is.character)], 
    as.factor)
Ames_train_tidied$Totl_Area <- Ames_train_tidied$TotalBsmtSF+
  Ames_train$GrLivArea
```
##### 2. After careful observation, found that Total Area of the property can be added as new feature by adding features TotalBsmtSF and GrLivArea,
```{r , echo=FALSE, cache=FALSE,fig.width=20,fig.height=8, results=FALSE, warning=FALSE, comment=FALSE, warning=FALSE}
Ames_train_tidied %>%
  ggplot(aes(x=log_SalePrice,y=Totl_Area))+
  geom_point()
Ames_test$Totl_Area <- Ames_test$TotalBsmtSF+
  Ames_test$GrLivArea
```
  There is medium to strong linear positive relation between Totl_Area and log_SalePrice.
```{r , echo=FALSE, cache=FALSE, results=TRUE, warning=FALSE, comment=FALSE, warning=FALSE}
ColswithNA <- Ames_train[,colSums(is.na(
  Ames_train[ , 1:ncol(Ames_train)])) > 0]
ColswithNA$SalePrice <- Ames_train$SalePrice
ColswithNA$log_SalePrice <- Ames_train$log_SalePrice

missing <- naniar::miss_var_summary(ColswithNA)

missing <- missing %>%
  filter(n_miss >0) %>%
  mutate(pct_miss = round(pct_miss,1))

```
##### 3. Here is the missing value representation of numerical columns.
```{r , echo=FALSE, cache=FALSE,fig.width=20,fig.height=8, results=TRUE, warning=FALSE, comment=FALSE, warning=FALSE}
ggplot(missing,aes(reorder(variable, -pct_miss),y=pct_miss,fill=variable))+
  geom_bar(position="dodge", stat = "identity")+
  ggtitle("Seasonwise SalePrice from 2006-2010")
```
For the numeric columns with missing values, comparing correlation value here with target feature.
```{r , echo=FALSE, cache=FALSE, results=FALSE, warning=FALSE, comment=FALSE, warning=FALSE}

numeric_cols <- unlist(lapply(ColswithNA, is.numeric)) 
colsNA_Numeric <-  ColswithNA[ , numeric_cols]
colnames(colsNA_Numeric)
```
Considering to remove LotFrontage after careful observation. MasVnrArea and GarageYrBlt having good correlation with Saleprice.
```{r , echo=FALSE, cache=FALSE,fig.width=20,fig.height=8, results=FALSE, warning=FALSE, comment=FALSE,message=FALSE}  
ggcorr(colsNA_Numeric,label=T)
```

```{r , echo=FALSE, cache=FALSE,fig.width=20,fig.height=8, results=FALSE, warning=FALSE, comment=FALSE} 
ggpairs(colsNA_Numeric, 
        lower=list(continuous=wrap("smooth", colour="blue")),
        diag=list(continuous=wrap("barDiag", fill="darkblue")))
```
LotFrontage shows a moderate to weak positive connection with SalePrice, a low correlation, and a large number of outliers.This step also includes imputation of missing values for MasVnrArea. Since GarageYrBlt is associated with the Time feature, we may want to investigate further using additional temporal features.
```{r , echo=FALSE, cache=FALSE, results=FALSE, warning=FALSE, comment=FALSE, message=FALSE}
drop <- c("LotFrontage")
Ames_train_tidied = Ames_train_tidied[,!(names(Ames_train_tidied) %in% drop)]
Ames_test = Ames_test[,!(names(Ames_test) %in% drop)]
ncol(Ames_train_tidied)


#imputing MasVnrArea NA's with median values
Ames_train_tidied <- Ames_train_tidied %>%
  mutate(MasVnrArea=ifelse(is.na(MasVnrArea),
                           median(MasVnrArea, na.rm=TRUE),
                           MasVnrArea))
Ames_test <- Ames_test %>%
  mutate(MasVnrArea=ifelse(is.na(MasVnrArea),
                           median(MasVnrArea, na.rm=TRUE),
                           MasVnrArea))
```
Here we complete missing values imputation for Numeric columns with NA's.

##### 4.  Here comes the bigger categoric feature with missing values
   Considering only categorical columns which have missing values.
```{r , echo=FALSE, cache=FALSE, results=FALSE, warning=FALSE, comment=FALSE, message=FALSE}
#categorical columns with NA
categoric_cols <- ColswithNA %>% select(!colnames(colsNA_Numeric))
ncol(categoric_cols)
categoric_cols$SalePrice <- Ames_train$SalePrice
#There are 16 categorical columns
#Further checking how much percentage of missing values we have here.
#which would help us to drop columns if the missing values are 50%
#Missing value ratio in categoric features.

missing <- naniar::miss_var_summary(categoric_cols)

missing <- missing %>%
  filter(n_miss >0) %>%
  mutate(pct_miss = round(pct_miss,1))
```
Barplot of missing percentage for each of the categoric feature is shown here. There are 3 categorical features which shows more than 80% missing value
```{r , echo=FALSE, cache=FALSE,fig.width=20,fig.height=8, results=TRUE, warning=FALSE, comment=FALSE, message=FALSE}
ggplot(missing,aes(reorder(variable, -pct_miss),y=pct_miss,fill=variable))+
  geom_bar(position="dodge", stat = "identity")+
  ggtitle("Seasonwise SalePrice from 2006-2010")
catcols_withNA_80P <- categoric_cols[, colSums(
  is.na(categoric_cols)) >= nrow(categoric_cols) * 0.8]
colnames(catcols_withNA_80P)
```

```{r , echo=FALSE, cache=FALSE, results=FALSE, warning=FALSE, comment=FALSE, message=FALSE}
catcols_withNA_80P$SalePrice <- Ames_train$SalePrice
```

```{r , echo=FALSE, cache=FALSE,fig.width=20,fig.height=8, results=TRUE, warning=FALSE, comment=FALSE, message=FALSE}
catcols_withNA_80P %>% 
  gather(variable, value,-SalePrice) %>%
  ggplot(aes(factor(value), SalePrice, fill = factor(value))) +
  geom_boxplot() +
  facet_wrap(~variable, scales = "free_x", nrow = 1, strip.position = "bottom") +
  theme(panel.spacing = unit(0, "lines"),
        panel.border = element_rect(fill = NA),
        strip.background = element_blank(),
        axis.title.x = element_blank(),
        legend.position = "none",
        strip.placement = "outside")
```
Alley, PoolQC, Fence, MiscFeature, and FireplaceQu exhibit outliers and have NA values that are equal to or greater than 50%. Additionally, there isn't much variation in SalePrice. We can eliminate these variables immediately.

```{r , echo=FALSE, cache=FALSE, results=FALSE, warning=FALSE, comment=FALSE, message=FALSE}
drop <- c("Alley","PoolQC","Fence","MiscFeature")
Ames_train_tidied = Ames_train_tidied[,!(names(Ames_train_tidied) %in% drop)]
Ames_test = Ames_test[,!(names(Ames_test) %in% drop)]
ncol(Ames_train_tidied)
ncol(Ames_test)
```

##### 5. collecting all Temporal(Time/Year) related features for analysis here.After careful observation of all temoral variable, decided to convert GarageYrBuilt,Yrbuilt,YearRemod so that they represent the age of the garage and the property. As the YearSold already showing for which and all year we have data, taking difference between YrSold respectively with each feature here gives the age of the property and Garage area.
```{r , echo=FALSE, cache=FALSE, results=FALSE, warning=FALSE, comment=FALSE, message=FALSE}
Cols_Year <- Ames_train %>% 
  dplyr:: select(grep("Yr", names(Ames_train)),
                 grep("Year", names(Ames_train)),
                 grep("*Yr", names(Ames_train)))

for(cols in colnames(Cols_Year)){
  if(cols!="YrSold"){
    Ames_train_tidied[,cols] <- Ames_train_tidied$YrSold-Ames_train_tidied[,cols]
    Ames_test[,cols] <- Ames_test$YrSold-Ames_test[,cols]}}

Cols_Year <- Ames_train_tidied %>% 
  dplyr:: select(grep("Yr", names(Ames_train_tidied)),
                 grep("Year", names(Ames_train_tidied)),
                 grep("*Yr", names(Ames_train_tidied)))

Cols_Year$SalePrice <- Ames_train_tidied$SalePrice
Cols_Year$log_SalePrice <- Ames_train_tidied$log_SalePrice
```

```{r , echo=FALSE, cache=FALSE, fig.width=20,fig.height=8,results=TRUE, warning=FALSE, comment=FALSE, message=FALSE}
ggpairs(Cols_Year[-2], 
        lower=list(continuous=wrap("smooth", colour="green")),
        diag=list(continuous=wrap("barDiag", fill="darkgreen")))
```
The Sale Price is declining as the property matures in terms of the garage or the building itself. 

Given that the three yearwise columns' correlation values are all quite high, it appears that they all have a negative, strong link with sale price and are therefore significant in forecasting sale price.


Here checking for YrSold and SalePrice relationship.
```{r , echo=FALSE, cache=FALSE, results=FALSE, warning=FALSE, comment=FALSE, message=FALSE}
#checking the Sale Price versus YearSold
Yrsold_SP <- Cols_Year %>% group_by(YrSold) %>%
  summarise(Median_SP=median(SalePrice))
```

```{r , echo=FALSE, cache=FALSE, fig.width=20,fig.height=8,results=TRUE, warning=FALSE, comment=FALSE, message=FALSE}
ggplot(Yrsold_SP, aes(x=YrSold, y=Median_SP)) +
  geom_line(color="red")+
  geom_point()+
  ggtitle("Yearwise sold properties Median Sale Price")+
  xlab("YearSold")+
  ylab("Median of Sale Price")
```
Sale Price declining between 2006 and 2010 is another noteworthy finding to take into account..

Property values typically increase as a result of the community and amenities. However, the building's general exterior and interior quality also deteriorates with increasing property age.

We might also make the intriguing column SeasonSold in this instance. Weather-related possibilities must exist; perhaps SalePrice is a determining factor. While we build this SeasonSold column here, MonthSold may be removed.

```{r , echo=FALSE, cache=FALSE, results=FALSE, warning=FALSE, comment=FALSE, message=FALSE}
#Using moSold, SeasonSold column is been created .
Winter <- c(1,2,12)
autumn <- c(9:11)
summmer <- c(6:8)
spring <- c(3:5)

Ames_train_tidied <- Ames_train_tidied %>% mutate(
  SeasonSold = ifelse(MoSold %in% autumn, "autumn",
                                ifelse(MoSold %in% Winter, "Winter",
                                       ifelse(MoSold %in% summmer, "summmer",
                                              "Spring"))))
Ames_test <- Ames_test %>% mutate(
  SeasonSold = ifelse(MoSold %in% autumn, "autumn",
                      ifelse(MoSold %in% Winter, "Winter",
                             ifelse(MoSold %in% summmer, "summmer",
                                    "Spring"))))

#dropping MoSold as I created SeasonSold
drop <- c("MoSold")
Ames_train_tidied = Ames_train_tidied[,!(names(Ames_train_tidied) %in% drop)]
Ames_test = Ames_test[,!(names(Ames_test) %in% drop)]
ncol(Ames_train_tidied)

Cols_Year$SeasonSold <- Ames_train_tidied$SeasonSold

Cols_Year <- Cols_Year %>% group_by(YrSold,SeasonSold) %>%
  mutate(median_SP=median(SalePrice))
```

```{r , echo=FALSE, cache=FALSE, fig.width=20,fig.height=8,results=TRUE, warning=FALSE, comment=FALSE, message=FALSE}
ggplot(Cols_Year,aes(x=YrSold,y=SalePrice,fill=SeasonSold))+
  geom_bar(position="dodge", stat = "identity")+
  ggtitle("Seasonwise SalePrice from 2006-2010")
```
SeasonSold has very big impact on the SalePrice  this was particularly true in 2007, when winter and spring had the highest sale prices for the houses sold, followed by autumn and spring.With the exception of 2007, every season when properties were sold between 2006 and 2010 generally followed the same pattern.

Imputation of Temporal variable. Filling missing values with median is done later.
```{r , echo=FALSE, cache=FALSE, results=FALSE, warning=FALSE, comment=FALSE, message=FALSE}
colSums(is.na(Cols_Year))

#Imputing GarageYrBlt NA's

Ames_train_tidied <- Ames_train_tidied %>%
  mutate(GarageYrBlt=ifelse(is.na(GarageYrBlt),
                           median(GarageYrBlt, na.rm=TRUE),
                           GarageYrBlt))

Ames_test <- Ames_test %>%
  mutate(GarageYrBlt=ifelse(is.na(GarageYrBlt),
                            median(GarageYrBlt, na.rm=TRUE),
                            GarageYrBlt))

```
Till now explored with whether any new features can be created. And also dealt with missing values.

Imputation and feature selection for features with missing values.
Keeping in mind that still categorical columns are not imputed until now.

##### 5. Here analyzing numerical columns without missing values. Whether any feature could be dropped or not is decided based on correlation values with SalePrice.

```{r , echo=FALSE, cache=FALSE, results=FALSE, warning=FALSE, comment=FALSE, message=FALSE}

#Numeric columns Analysis
numeric_cols <- unlist(lapply(Ames_train_tidied, is.numeric)) 
Cols_allNumrc <-  Ames_train_tidied[ , numeric_cols]
ncol(Cols_allNumrc)
colnames(Cols_allNumrc)

drop <- c("GarageYrBlt","YrSold","YearBuilt" ,
          "YearRemodAdd","LotFrontage","MasVnrArea")
Cols_allNumrc = Cols_allNumrc[,!(names(Cols_allNumrc) %in% drop)]
```

```{r , echo=FALSE, cache=FALSE, fig.width=25,fig.height=12,results=TRUE, warning=FALSE, comment=FALSE, message=FALSE}
#Finding correlation 
ggcorr(Cols_allNumrc,label = T)
```
Most of the features which are having low correlation that is less than
+/-0.5 with SalePrice could be dropped here.
```{r , echo=FALSE, cache=FALSE, results=FALSE, warning=FALSE, comment=FALSE, message=FALSE}
 
#These columns could be dropped as there is no much relation with the target variable.Removing Id column also

drop <- c("MiscVal","PoolArea","ScreenPorch",
          "EnclosedPorch","OpenPorchSF","WoodDeckSF",
          "KitchenAbvGr","BedroomAbvGr","HalfBath",
          "BsmtHalfBath","BsmtFullBath","LowQualFinSF",
          "X2ndFlrSF","BsmtUnfSF","BsmtFinSF2","BsmtUnfSF",
          "X3SsnPorch","OverallCond","LotArea","MSSubClass")

#Analysing them once before dropping
plot_drop <- Cols_allNumrc %>% 
  select(drop)
plot_drop$SalePrice <- Ames_train_tidied$SalePrice
```

```{r , echo=FALSE, cache=FALSE,fig.keep = 'none', warning=FALSE, comment=FALSE, message=FALSE}
ggpairs(plot_drop, 
        lower=list(continuous=wrap("smooth", colour="green")),
        diag=list(continuous=wrap("barDiag", fill="darkgreen")))
```

As we could see the feature having less than 0.5 correlation  SalePrice shows clear info on the scatterplot which is not printed here. When Analysed further segregating them to discrete and continuous features, got more insights on them.
```{r , echo=FALSE, cache=FALSE, results=FALSE, warning=FALSE, comment=FALSE, message=FALSE}

#Dropping the numeric cols which are having less tha +/-0.5 correlation with SalePrice after Analysis.

Ames_train_tidied = Ames_train_tidied[,!(names(Ames_train_tidied) %in% drop)]
ncol(Ames_train_tidied)
Ames_test = Ames_test[,!(names(Ames_test) %in% drop)]
ncol(Ames_train_tidied)
Cols_allNumrc = Cols_allNumrc[,!(names(Cols_allNumrc) %in% drop)]
colnames(Cols_allNumrc)

#segregating rest of the numeric columns to continuous and discrete elements
colnames(Cols_allNumrc)
disc_cols <- c("OverallQual",
               "FullBath",
               "TotRmsAbvGrd","Fireplaces",
               "GarageCars")

cols_discrete <- data.frame(Cols_allNumrc) %>% select(disc_cols)
cols_discrete$SalePrice <- Ames_train$SalePrice

ncol(cols_discrete)
#There are 5 columns which are discrete.
```


```{r , echo=FALSE, cache=FALSE, fig.width=20,fig.height=8, results=TRUE, warning=FALSE, comment=FALSE, message=FALSE}
ggpairs(cols_discrete, 
        lower=list(continuous=wrap("smooth", colour="blue")),
        diag=list(continuous=wrap("barDiag", fill="darkblue")))

```

```{r , echo=FALSE, cache=FALSE,fig.width=20,fig.height=8, results=TRUE, warning=FALSE, comment=FALSE, message=FALSE}
cols_discrete %>% 
  gather(variable, value,-SalePrice) %>%
  ggplot(aes(factor(value), SalePrice, fill = factor(value))) +
  geom_boxplot() +
  facet_wrap(~variable, scales = "free_x", nrow = 1, strip.position = "bottom") +
  theme(panel.spacing = unit(0, "lines"),
        panel.border = element_rect(fill = NA),
        strip.background = element_blank(),
        axis.title.x = element_blank(),
        legend.position = "none",
        strip.placement = "outside")
```
Complete bathroom, garage cars, and general quality all have a high, positive correlation with sale price..
Fireplaces have a favorable increase in SalePrice, and TotRmsAbvGrd likewise shows a positive
strong relation and much variability with SalePrice

Now dealing with continuous variables with exploratory data analysis.
```{r , echo=FALSE, cache=FALSE, results=FALSE, warning=FALSE, comment=FALSE, message=FALSE}
#continuous columns
cols_continuous <- Cols_allNumrc %>% select(!disc_cols)
ncol(cols_continuous)
```

```{r , echo=FALSE, cache=FALSE, fig.width=20,fig.height=8,results=TRUE, warning=FALSE, comment=FALSE, message=FALSE}
ggpairs(cols_continuous, 
        lower=list(continuous=wrap("smooth", colour="yellow")),
        diag=list(continuous=wrap("barDiag", fill="orange")))
```

All these continuous features has very strong positive linear relation with SalePrice. All the numerical columns analysis completes here.

##### 6. Now considering all the categorical variables which are not having missing values.And even those with missing values are imputed here in this step.
```{r , echo=FALSE, cache=FALSE, results=FALSE, warning=FALSE, comment=FALSE, message=FALSE}

#features in categorical has to be analysed.
cols_categoric <- Ames_train_tidied %>% select_if(negate(is.numeric))
colnames(cols_categoric)
cols_categoric$SalePrice  <- Ames_train_tidied$SalePrice

#Fireplace can be removed as it has highest percentage of missing value Have grouped into 4 groups just for visualisation purpose.
#1st category will have categories what 
#I feel are important from the buyers perspective
cols_cat1_imp1 <- c("Street","LotShape","LandContour",  
              "Utilities","Neighborhood", "BldgType","HouseStyle")

cat1_imp1  <- Ames_train %>% select(cols_cat1_imp1)
cat1_imp1$SalePrice <- Ames_train$SalePrice
cat1_imp1$YrSold <- Ames_train_tidied$YrSold
```

Given the abundance of category features, they are divided into internal and external key categories as well as miscellaneous categories. This is purely for the purpose of visualization.
```{r , echo=FALSE, cache=FALSE,fig.keep = 'none', warning=FALSE, comment=FALSE, message=FALSE}
cat1_imp1 %>% 
  gather(variable, value,-SalePrice) %>%
  ggplot(aes(factor(value), SalePrice, fill = factor(value))) +
  geom_boxplot() +
  facet_wrap(~variable, scales = "free_x", nrow = 1, strip.position = "bottom") +
  theme(panel.spacing = unit(0, "lines"),
        panel.border = element_rect(fill = NA),
        strip.background = element_blank(),
        axis.title.x = element_blank(),
        legend.position = "none",
        strip.placement = "outside")


```

Utilities could be removed because of no variability with SalePrice and many outliers when checked with boxplot which is not printed here.
```{r , echo=FALSE, cache=FALSE, results=FALSE, warning=FALSE, comment=FALSE, message=FALSE}
cols_cat1_imp2 <- c("RoofStyle","RoofMatl","MasVnrType","ExterQual",
                    "ExterCond","BsmtCond","KitchenQual","GarageQual",
                    "SaleType","SaleCondition")

cat1_imp2  <- Ames_train %>% select(cols_cat1_imp2)
cat1_imp2$SalePrice <- Ames_train$SalePrice
cat1_imp2$YrSold <- Ames_train_tidied$YrSold

```

```{r , echo=FALSE, cache=FALSE,fig.keep = 'none', warning=FALSE, comment=FALSE, message=FALSE}
cat1_imp2 %>% 
  gather(variable, value,-SalePrice) %>%
  ggplot(aes(factor(value), SalePrice, fill = factor(value))) +
  geom_boxplot() +
  facet_wrap(~variable, scales = "free_x", nrow = 1, strip.position = "bottom") +
  theme(panel.spacing = unit(0, "lines"),
        panel.border = element_rect(fill = NA),
        strip.background = element_blank(),
        axis.title.x = element_blank(),
        legend.position = "none",
        strip.placement = "outside")
```

  Since there is little variation in this case when compared to SalePrice, four columns—BsmtCond, ExterCond, RoofStyle, and RoofMatl—can be eliminated at this time.once the boxplot, which is not printed here, has been checked.
```{r , echo=FALSE, cache=FALSE, results=FALSE, warning=FALSE, comment=FALSE, message=FALSE}
drop <- c("BsmtCond","ExterCond", "RoofStyle","RoofMatl",
          "HouseStyle", "Street","Utilities" )
Ames_test = Ames_test[,!(names(Ames_test) %in% drop)]
Ames_train_tidied = Ames_train_tidied[,!(names(Ames_train_tidied) %in% drop)]

cols_cat2_misc <- c("MSZoning","LotConfig","LandSlope","Condition1","Condition2",
                    "Heating","HeatingQC","CentralAir","Electrical","Functional",
                    "Foundation","TotRmsAbvGrd", "FireplaceQu")

cat2_misc <-  Ames_train_tidied %>% select(cols_cat2_misc)
cat2_misc$SalePrice <- Ames_train_tidied$SalePrice
cat2_misc$YrSold <- Ames_train_tidied$YrSold
```

```{r , echo=FALSE, cache=FALSE,fig.keep = 'none', warning=FALSE, comment=FALSE, message=FALSE}
cat2_misc %>% 
  gather(variable, value,-SalePrice) %>%
  ggplot(aes(factor(value), SalePrice, fill = factor(value))) +
  geom_boxplot() +
  facet_wrap(~variable, scales = "free_x", nrow = 1, strip.position = "bottom") +
  theme(panel.spacing = unit(0, "lines"),
        panel.border = element_rect(fill = NA),
        strip.background = element_blank(),
        axis.title.x = element_blank(),
        legend.position = "none",
        strip.placement = "outside")
```

In comparison to other features here, TotRmsAbvGrd has very good variability.
Here, I'm dropping everything but TotRmsAbvGrd.once the boxplot, which is not printed here, has been checked.

```{r , echo=FALSE, cache=FALSE, results=FALSE, warning=FALSE, comment=FALSE, message=FALSE}
drop <- c("MSZoning","LotConfig","LandSlope","Condition1","Condition2",
          "Heating","HeatingQC","CentralAir","Electrical","Functional",
          "Foundation", "FireplaceQu")
Ames_test = Ames_test[,!(names(Ames_test) %in% drop)]
Ames_train_tidied = Ames_train_tidied[,!(names(Ames_train_tidied) %in% drop)]


#External category features
cols_cat3_Ext <- c("PavedDrive", "GarageType","GarageFinish","GarageCond",
                   "BsmtQual","BsmtExposure","BsmtFinType1", 
                   "BsmtFinType2","Exterior1st","Exterior2nd")

cat3_Ext <-  Ames_train_tidied %>% select(cols_cat3_Ext)
cat3_Ext$SalePrice <- Ames_train_tidied$SalePrice
cat3_Ext$YrSold <- Ames_train_tidied$YrSold
```

```{r , echo=FALSE, cache=FALSE,fig.width=23,fig.height=10, results=FALSE, warning=FALSE, comment=FALSE, message=FALSE}
cat3_Ext %>% 
  gather(variable, value,-SalePrice) %>%
  ggplot(aes(factor(value), SalePrice, fill = factor(value))) +
  geom_boxplot() +
  facet_wrap(~variable, scales = "free_x", nrow = 1, strip.position = "bottom") +
  theme(panel.spacing = unit(0, "lines"),
        panel.border = element_rect(fill = NA),
        strip.background = element_blank(),
        axis.title.x = element_blank(),
        legend.position = "none",
        strip.placement = "outside")
```

BsmtQual looks better related compared to all other features here. Removing all other except BsmtQual here. Once we select features from categorical features, Imputation for all the categorical features done at this stage.

```{r , echo=FALSE, cache=FALSE, results=FALSE, warning=FALSE, comment=FALSE, message=FALSE}

drop <- c("PavedDrive", "GarageType","GarageFinish","GarageCond",
                   "BsmtExposure","BsmtFinType1", 
                   "BsmtFinType2","Exterior1st","Exterior2nd")
Ames_test = Ames_test[,!(names(Ames_test) %in% drop)]
Ames_train_tidied = Ames_train_tidied[,!(names(Ames_train_tidied) %in% drop)]


#imputing missing values of categorical variables with replacing NA to Missing.

replace_factor_na <- function(x){
  x <- as.character(x)
  x <- if_else(is.na(x), "Missing", x)
  x <- as.factor(x)
}

Ames_train_tidied <- Ames_train_tidied %>%
  mutate_if(is.factor, replace_factor_na)

Ames_test <- Ames_test %>%
  mutate_if(is.factor, replace_factor_na)

colSums(is.na(Ames_train_tidied))
colSums(is.na(Ames_test))

for (cols in which(sapply(Ames_test, is.numeric))) {
  for (row in which(is.na(Ames_test[, cols]))) {
    Ames_test[row, cols] <- median(Ames_test[[cols]],
                                           na.rm = TRUE)}}

#Missing values imputation completed
colnames(Ames_train_tidied)
colnames(Ames_test)
```

#### 6. Further Preprocessing: 

##### Further Preprocessing and Exploratory data analysis done at this step. Deeper understanding and finding some of the solutions to the problems identified at the beginning of this project.
During the first stage of data analysis , the decissions are taken carefuly. so that it doesnt take much of the effort here.

##### Problem 1: Identify which suburb/location had the biggest growth in SalePrice by plotting and examining the sale prices cross different suburbs.Has there been a trend on the type of house bought and had big hike in Sale price from 2006 to 2010.
```{r , echo=FALSE, cache=FALSE,fig.width=23,fig.height=10, results=TRUE, warning=FALSE, comment=FALSE, message=FALSE}

Ames_train_tidied %>%
  group_by(Neighborhood,YrSold) %>%
  summarise(med_SP=median(log_SalePrice)) %>% 
  ggplot(aes(YrSold,med_SP,color=factor(Neighborhood))) +
  geom_point() +
  geom_line() +
  facet_wrap(~Neighborhood)+
  ggtitle("SalePrice based on GarageQual and Garage Age") +
  theme(plot.title = element_text(hjust = 0.5)) +
  ylab("Sale Price $")
```

1) The price of the Somerst suburb decreased in 2010 after rising until 2009.
2) Although Noridge's sale price has been rising as a trend, it has declined in 2009.
3) NAmes shows a yearly tendency of minor increase.
So there must be other features which are affecting SalePrice here along with 
Neighborhood(suburb/location) of the property.


##### Problem 2:	Analyze a possible pattern of SalePrice vs YrSold/MoSold, LotArea and/or some other variables which can reasonably be included considering Totl_Area instead of LotArea, SeasonSold instead of MonthSold

```{r , echo=FALSE, cache=FALSE,fig.width=20,fig.height=10, results=TRUE, warning=FALSE, comment=FALSE, message=FALSE}
Ames_train_tidied %>%
  ggplot(aes(x = SeasonSold , y = Totl_Area,col = log_SalePrice)) +
  geom_line()+
  geom_point() +
  facet_wrap(~ YrSold , scales = "free") +
  scale_color_continuous(name="Sale Price in $",
                         low="yellow",
                         high="darkblue")+
  theme_light()
```
1) The summer of 2006 saw an exceptional surge in the sale price that appeared to be an outlier; nonetheless, the maximum sale price for all houses sold with larger total area (total basement area and ground living) was reached throughout the season at $4000.
2) In 2008, all seasons saw extremely low sale prices, especially for large estates in Totl Area. The average log_sale price was approximately $4,000.
3) In 2009, all seasons saw maxima for the properties with larger Totl_Area of over $5,000.


##### Problem 3: Whether SaleCondition has any impact on the SalePrice,Explain with Data Analysis and give insights on whether this feature needs to be considered.

```{r , echo=FALSE, cache=FALSE, fig.width=20,fig.height=8,results=TRUE, warning=FALSE, comment=FALSE, message=FALSE}

ggplot(Ames_train_tidied,aes(x=SalePrice))+
  geom_histogram(aes(fill=SaleCondition),
                     position=position_stack(), binwidth = 10000,
                 colour="yellow")+
  ggtitle("Median Saleprice based on SaleCondition")
```

```{r , echo=FALSE, cache=FALSE, fig.width=20,fig.height=8,results=TRUE, warning=FALSE, comment=FALSE, message=FALSE}
ggplot(Ames_train_tidied,aes(SaleCondition,SalePrice,fill=SaleCondition))+
  geom_boxplot()+
  ggtitle("Median Saleprice across SaleCondition")
```

1) SalePrice and SaleCondition unquestionably have a relationship.
2) Typical Transaction a lot of outliers and varied. 
3) The sale prices of family sales between relatives decreased significantly. 
4) In a similar vein, an abnormally large sale could result from a swap, short sale, or foreclosure.


##### Problem 4 : Any change in SalePrice over the period from 2006 to 2010 based on GarageQual
```{r , echo=FALSE, cache=FALSE, fig.width=20,fig.height=8,results=TRUE, warning=FALSE, comment=FALSE, message=FALSE}
  ggplot(Ames_train_tidied, aes(x = YrSold,
                                y = log_SalePrice, fill = factor(GarageQual))) +
  geom_bar(position = "stack", stat = "identity") +
  ggtitle("Property sale Price in $ by GarageQuality") +
  ylab("Sale Price $") +
  xlab("Year Sold") +
  theme(plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
        axis.title.x = element_text(size = 12, hjust = 0.5),
        axis.title.y = element_text(size = 12, hjust = 0.5),
        legend.position ="right",
        axis.text.x = element_text(angle = 30,  
                                   vjust = 1, 
                                   size = 6, 
                                   hjust = 1)) +
  scale_fill_discrete(name="Garage Quality")
```
1) The SalePrice did, in fact, vary over the years. From 2006 to 2009
2) The average quality garage properties' log_SalePrice was approximately $3,500.
3) In 2010, the Sale Price of even very good grade garages unexpectedly dropped; this could have been brought on by the garage's advanced age.


##### Problem 5(a):Over the years.How the SalePrice changed based on Neighborhood and BldgType .Explain
```{r , echo=FALSE, cache=FALSE,fig.width=20,fig.height=8, results=TRUE, warning=FALSE, comment=FALSE, message=FALSE}
  Ames_train_tidied %>%
    group_by(Neighborhood, BldgType,YrSold) %>%
    summarise(med_SP= median(SalePrice)) %>%
    ggplot(aes(YrSold, med_SP, fill = BldgType)) +
    geom_bar(position="dodge", stat = "identity") +
    facet_wrap( ~ Neighborhood ) +
    ggtitle("SalePrice from 2006-2010 based on Neighborhood and BldgType") +
    theme(plot.title = element_text(hjust = 0.5)) +
    ylab("Sale Price $")
```  
1)A number of neighborhoods, including BrkSide, ClearCr, Gilbert, IDOTRR, NoRidge, NWAmes, Timber, NRidgHt, StoneBr, and others, have sold dwellings of type 1Fam.
2) From 2006 to 2010, the only 1Fam building type in the ClearCr Gilbert, Timber NoRidge location seemed stylish. Every year, the sale price is essentially the same.
3) The building type TwnhsE, 1Fam in Suburban Blmngtn CollgCr StoneBr, Somerst, appears to be fashionable. From 2006 to 2010, the sale price for both types of buildings is about the same.
  

##### Problem 5(b):Do you think we could get good linear model with just considering Overall quality as a stand alone parameter for Sale Price prediction? Give thoughts

```{r , echo=FALSE, cache=FALSE,fig.width=20,fig.height=8, results=TRUE, warning=FALSE, comment=FALSE, message=FALSE}
ggplot(Ames_train_tidied,aes(x=OverallQual,y=log_SalePrice))+
  geom_point()
```

```{r , echo=TRUE, cache=FALSE, results=FALSE, warning=FALSE, comment=FALSE, message=FALSE}
model <- lm(Ames_train_tidied$log_SalePrice ~ Ames_train_tidied$OverallQual,
            Ames_train_tidied)
summary(model)
```
Despite having an RMSE of 0.2303 and an R-squared of 0.6678, the linear model cannot be deemed satisfactory based solely on the OverallQuality.

##### The problem 5(b) will be solved once fitting the model is completed.

#### 7. Modeling:

Here considered features with strong correlation and good variability to SalePrice.
```{r , echo=FALSE, cache=FALSE, results=FALSE, warning=FALSE, comment=FALSE, message=FALSE}

####################----------Linear Modeling------------######################

#Removing Id column from both train and test data set
drop <- c("Id")
Ames_train_tidied = Ames_train_tidied[,!(names(Ames_train_tidied) %in% drop)]
Ames_test = Ames_test[,!(names(Ames_test) %in% drop)]
ncol(Ames_train_tidied)
ncol(Ames_test)

colnames(Ames_train_tidied)
colnames(Ames_test)


model_Ames_train_tidied<- lm(Ames_train_tidied$log_SalePrice ~ Ames_train_tidied$LotShape + 
                                 Ames_train_tidied$Neighborhood +
                                 Ames_train_tidied$BldgType +
                                 Ames_train_tidied$OverallQual +
                                 Ames_train_tidied$FullBath +
                                 Ames_train_tidied$KitchenQual +
                                 Ames_train_tidied$TotRmsAbvGrd + 
                                 Ames_train_tidied$Fireplaces+
                                 Ames_train_tidied$GarageCars+
                                 Ames_train_tidied$GarageArea +
                                 Ames_train_tidied$YrSold +
                                 Ames_train_tidied$Totl_Area +
                                 Ames_train_tidied$SeasonSold,Ames_train_tidied)
```

I was able to obtain the R square=0.8599 and RMSE=0.1519 values using the first fitted model.
```{r , echo=TRUE, cache=FALSE, results=FALSE, warning=FALSE, comment=FALSE, message=FALSE}
summary(model_Ames_train_tidied) 
```

```{r , echo=FALSE, cache=FALSE, results=FALSE, warning=FALSE, comment=FALSE, message=FALSE}

mode2_Ames_train_tidied<- lm(Ames_train_tidied$log_SalePrice ~ Ames_train_tidied$LotShape + 
                                 Ames_train_tidied$LandContour +
                                 Ames_train_tidied$Neighborhood +
                                 Ames_train_tidied$BldgType +
                                 Ames_train_tidied$OverallQual +
                                 Ames_train_tidied$YearBuilt +
                                 Ames_train_tidied$YearRemodAdd +
                                 Ames_train_tidied$MasVnrType +
                                 Ames_train_tidied$MasVnrArea +
                                 Ames_train_tidied$ExterQual +
                                 Ames_train_tidied$BsmtQual +
                                 Ames_train_tidied$BsmtFinSF1 +
                                 Ames_train_tidied$TotalBsmtSF +
                                 Ames_train_tidied$X1stFlrSF +
                                 Ames_train_tidied$GrLivArea +
                                 Ames_train_tidied$FullBath +
                                 Ames_train_tidied$KitchenQual +
                                 Ames_train_tidied$TotRmsAbvGrd + 
                                 Ames_train_tidied$Fireplaces+
                                 Ames_train_tidied$GarageYrBlt+
                                 Ames_train_tidied$GarageCars+
                                 Ames_train_tidied$GarageArea +
                                 Ames_train_tidied$YrSold +
                                 Ames_train_tidied$Totl_Area ,Ames_train_tidied)
```
It was possible to obtain the values of R square = 0.8772 and RMSE = 0.1431 using the second fitted model.
```{r , echo=FALSE, cache=FALSE, results=FALSE, warning=FALSE, comment=FALSE, message=FALSE}
  summary(mode2_Ames_train_tidied) 
```

```{r , echo=FALSE, cache=FALSE, results=FALSE, warning=FALSE, comment=FALSE, message=FALSE}
  
mode3_Ames_train_tidied<- lm(Ames_train_tidied$log_SalePrice ~ Ames_train_tidied$OverallQual + 
                                 Ames_train_tidied$YearBuilt +
                                 Ames_train_tidied$Totl_Area +
                                 Ames_train_tidied$YearRemodAdd +
                                 Ames_train_tidied$MasVnrArea +
                                 Ames_train_tidied$BsmtFinSF1 +
                                 Ames_train_tidied$TotalBsmtSF +
                                 Ames_train_tidied$GrLivArea +
                                 Ames_train_tidied$X1stFlrSF +
                                 Ames_train_tidied$FullBath +
                                 Ames_train_tidied$Fireplaces +
                                 Ames_train_tidied$GarageYrBlt +
                                 Ames_train_tidied$YrSold +
                                 Ames_train_tidied$BldgType +
                                 Ames_train_tidied$OverallQual +
                                 Ames_train_tidied$MasVnrType +
                                 Ames_train_tidied$ExterQual + 
                                 Ames_train_tidied$KitchenQual+
                                 Ames_train_tidied$GarageQual+
                                 Ames_train_tidied$SaleType+
                                 Ames_train_tidied$SaleCondition,Ames_train_tidied)
```
I was able to obtain the R square = 0.8507 and RMSE = 0.1567 values using the third fitted model.
```{r , echo=TRUE, cache=FALSE, results=FALSE, warning=FALSE, comment=FALSE, message=FALSE}
  summary(mode3_Ames_train_tidied)
```
 
```{r , echo=FALSE, cache=FALSE, results=FALSE, warning=FALSE, comment=FALSE, message=FALSE} 
mode4_Ames_train_tidied<- lm(Ames_train_tidied$log_SalePrice ~ Ames_train_tidied$LotShape + 
                                 Ames_train_tidied$LandContour +
                                 Ames_train_tidied$Neighborhood +
                                 Ames_train_tidied$BldgType +
                                 Ames_train_tidied$OverallQual +
                                 Ames_train_tidied$YearBuilt +
                                 Ames_train_tidied$YearRemodAdd +
                                 Ames_train_tidied$MasVnrType +
                                 Ames_train_tidied$MasVnrArea +
                                 Ames_train_tidied$ExterQual +
                                 Ames_train_tidied$BsmtQual +
                                 Ames_train_tidied$BsmtFinSF1 +
                                 Ames_train_tidied$TotalBsmtSF +
                                 Ames_train_tidied$X1stFlrSF +
                                 Ames_train_tidied$GrLivArea +
                                 Ames_train_tidied$FullBath +
                                 Ames_train_tidied$KitchenQual +
                                 Ames_train_tidied$TotRmsAbvGrd + 
                                 Ames_train_tidied$Fireplaces+
                                 Ames_train_tidied$GarageYrBlt+
                                 Ames_train_tidied$GarageCars+
                                 Ames_train_tidied$GarageArea +
                                 Ames_train_tidied$GarageQual +
                                 Ames_train_tidied$YrSold +
                                 Ames_train_tidied$SaleType +
                                 Ames_train_tidied$SaleCondition +
                                 Ames_train_tidied$Totl_Area +
                                 Ames_train_tidied$SeasonSold,Ames_train_tidied)
```
##### mode4_Ames_train_tidied is the best fit with RMSE=0.1401 # and R^2=0.884 compared to other fitted models for test data set-> lm

There are few outliers as following residual shows in the fourth model.
```{r , echo=TRUE, cache=FALSE, results=TRUE, warning=FALSE, comment=FALSE, message=FALSE}
  outlierTest(mode4_Ames_train_tidied)
  #There are few outliers.
```
was successful in obtaining the R square=0.884 and RMSE=0.1401 values using the third fitted model.
```{r , echo=TRUE, cache=FALSE, results=FALSE, warning=FALSE, comment=FALSE, message=FALSE}
  summary(mode4_Ames_train_tidied) 
``` 
The best fitted model's linear regression, residual, and cook's distance plot are shown here.
```{r , echo=FALSE, cache=FALSE, results=TRUE, fig.width=20,fig.height=8, warning=FALSE, comment=FALSE, message=FALSE}
  plot(mode4_Ames_train_tidied,1)
  plot(mode4_Ames_train_tidied,3)
``` 

I obtained a R square of -0.8973 and an RMSE of 0.1375 when I used the test dataset to make predictions, which is really close to the fitted model.

```{r , echo=FALSE, cache=FALSE, results=FALSE,fig.keep = 'none', warning=FALSE, comment=FALSE, message=FALSE}
plot(mode4_Ames_train_tidied,2)  
plot(mode4_Ames_train_tidied,4)
model_Ames_test<- lm(Ames_test$log_SalePrice ~ Ames_test$LotShape + 
                         Ames_test$LandContour +
                         Ames_test$Neighborhood +
                         Ames_test$BldgType +
                         Ames_test$OverallQual +
                         Ames_test$YearBuilt +
                         Ames_test$YearRemodAdd +
                         Ames_test$MasVnrType +
                         Ames_test$MasVnrArea +
                         Ames_test$ExterQual +
                         Ames_test$BsmtQual +
                         Ames_test$BsmtFinSF1 +
                         Ames_test$TotalBsmtSF +
                         Ames_test$X1stFlrSF +
                         Ames_test$GrLivArea +
                         Ames_test$FullBath +
                         Ames_test$KitchenQual +
                         Ames_test$TotRmsAbvGrd + 
                         Ames_test$Fireplaces+
                         Ames_test$GarageYrBlt+
                         Ames_test$GarageCars+
                         Ames_test$GarageArea +
                         Ames_test$GarageQual +
                         Ames_test$YrSold +
                         Ames_test$SaleType +
                         Ames_test$SaleCondition +
                         Ames_test$Totl_Area +
                         Ames_test$SeasonSold,Ames_test)
```

```{r , echo=TRUE, cache=FALSE, results=FALSE, warning=FALSE, comment=FALSE, message=FALSE}
  summary(model_Ames_test) 
```


```{r , echo=TRUE, cache=FALSE, fig.height=3,results=FALSE, warning=FALSE, comment=FALSE, message=FALSE}
  plot(model_Ames_test,1)
  plot(model_Ames_test,3)
```

Here is the comparision of RMSE values of both fitted model and test dataset.
```{r , echo=TRUE, cache=FALSE, results=TRUE, warning=FALSE, comment=FALSE, message=FALSE}
  rmse(mode4_Ames_train_tidied,Ames_train_tidied)
  rmse(model_Ames_test,Ames_test)
```
#### 9. Evalution:

##### predicting test dataset and plotting residuals

```{r , echo=FALSE, cache=FALSE, results=FALSE,fig.height=10,fig.keep = 'none' ,warning=FALSE, comment=FALSE, message=FALSE}  
plot(model_Ames_test,2)
plot(model_Ames_test,4)
  lmpred <- predict(model_Ames_test, newdata = Ames_test)
  lmdata <- Ames_test %>% mutate(y = log_SalePrice) %>% 
    mutate(ybar = lmpred) %>% mutate(diff = abs(y - ybar))
  
  badlmdata <- lmdata %>% filter(diff > 1.5) %>% arrange(desc(diff))
```

```{r , echo=FALSE, cache=FALSE, fig.width=20,fig.height=8,results=FALSE, warning=FALSE, comment=FALSE, message=FALSE}
  lmresiduals <- ggplot(lmdata, aes(x = y, y = y-ybar), col = diff) +
    geom_point(aes(x = y, y = y-ybar, color = diff)) +
    geom_point(data = badlmdata, colour="red") +
    scale_color_gradient(name = "|y - ybar|", limits = c(0, 1.5)) +
    geom_abline(slope = 1, intercept = 0) +
    xlab("y") +
    ylab("y-ybar") +
    ggtitle("Linear model residuals") +
    theme(plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
          axis.title.x = element_text(size = 12, hjust = 0.5),
          axis.title.y = element_text(size = 12, hjust = 0.5),
          legend.position = "right",
          legend.spacing.y = unit(0.5, 'cm'))
  
  lmresiduals
```  

Once again checking here on the RMSE value of residuals.
```{r , echo=FALSE, cache=FALSE, results=TRUE, warning=FALSE, comment=FALSE, message=FALSE}
  sqrt(mean((Ames_test$log_SalePrice - lmpred) ^ 2))
```


##### Problem 5: Use predictions from your final model to compare suburbs which have shown varying growth. Or, to identify which suburbs have been growing the most over the last few years.
  
#### According the fitted final model  the formula for prediction of SalePrice is as follows.
  
#### SalePrice = (-3.479e-02)*NeighborhoodBlueste+1.318e+01
#### SalePrice = ( -9.795e-02)*NeighborhoodBrDale+1.318e+01
#### SalePrice = (1.237e-01)*NeighborhoodVeenker+1.318e+01
#### and so on
  
#### so Based on the Neighbourhood value in test dataset SalePrice will be predicted. Similary the formulas could be derived with each and every dependent feature to predict SalePrice. The final formula would be as follows.

##### as Multiple linear models will follow the general form 
##### y = a1x1 + a2x2 + . + b
##### SalePrice = a1*Neighborhood + a2*OverallQual + b....and so on
##### while a1= feature1*coefficient+/-intercept and so on wheere feature=Neighborhood and coefficient and intercepts are the respective estimations using linear modeling. 


#### 10. Recomendations and Conclusion:
The relationship between house prices and the economy is an important factor
for predicting house prices. As per buyer and sellers concern Housing prices trends
are very important to study before making an investment, Hence it is directly
or indirectly related to current economic situation. Therefore it is important
to predict housing prices without bias to help both buyers and sellers make
their decisions

Through conducting data collection, including various data processing methods, and applying analytic approaches, I have identified a multiple linear regression model that is suitable for the dataset.

First, I cleaned up the train dataset and used it to generate 4 models. I could make four models based on the attributes' qualities.

##### The first model having R square value 0.8599 and RMSE: 0.1519.
##### The second model having R square value 0.8772 and RMSE: 0.1431. 
##### The third model having R square value 0.8507 and RMSE: 0.1567.
##### The fourth model having R square value 0.884 and RMSE: 0.1401 

Essentially, I chose the attributes based on their significant relationship to the SalePrice and their variability within the SalePrice distribution. In the end, I determine that the fourth model fits and predicts the values the best. Plotting residuals and predicting using the test dataset with the best model later, to see if the prediction based on the fitted model is close to the absolute value. Considered here is log_SalePrice rather than SalePrice. Using SalePrice's log transformation is the best option, since SalePrice's consideration for modeling yielded RMSE=30020 and R-squared=0.8654.


If there is any more research done, I would prefer to focus on multiple regression methods rather than just linear regression. That would assist me in examining and taking into account the remaining features that I neglected to include in my linear regression calculations.

#### 11. References:
##### References: Big vote of thanks to all the references mentioned below here. Without which I would have not successfully able to complete linear modelling for multivariable data set.


http://jse.amstat.org/v19n3/decock.pdf

http://stackoverflow.com/

http://www.cran.r-project.org 

http://www.stackoverflow.com

http://www.edx.org

http://www.rapidtables.com

https://rpubs.com/RobbyS/622233

https://scholarworks.calstate.edu/downloads/fx719m836

https://www.youtube.com/watch?v=wR4Xfwjr-3Y&list=LL&index=14

https://nycdatascience.com/